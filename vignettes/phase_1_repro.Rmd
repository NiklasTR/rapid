---
title: "Phase 1 - Reproducibility"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)
```

Loading packages

```{r}
library(tidyverse)
library(reticulate)
library(lubridate)

#clustering
library(umap)
library(dbscan)

#plotting
library(patchwork)
library(janitor)
library(platetools)
library(viridis)

# database
library(RPostgres)
library(pool)
library(RSQLite)
library(DBI)

library(here)
library(googlesheets4)
```

# Introduction

This vignette comprises 6 drug screens that were performed in 2019. The data contains the following experiments: 
* 3 runs performed based on the same sample (cRCRF1047T by PT-2HF4C) on independent timepoints from cryopreserved stocks by two operators (2 by Niklas Rindtorff, 1 by Mushriq Al-Jazrawe. 
* 2 runs with samples from the same patient, wich were collected a few weeks after therapy initiation with a 5FU based therapy (CCLF_cRCRF1066T and CCLF_cRCRF1050T by PT-2HF4C).
* 1 run with a sample from a different patient (CCLF_cRCRF1068T by PT-2INIB).

# Goal

The goals of this analysis are: 
* show behaviour of viability over time for untreated and control samples
* show robustness of sample viability behaviour over time between replicates
* show similar patterns in compound response for both biological replciates 
* explore differences in compound response between samples

This analysis does not:
* explore alternative & in-depth ways to describe drug response, for example by predicting timepoint and drug-concentration from a set of single cells
* explore phenotype differences between immune cells and cancer cells in the fluid samples. This data is available for some of the data.

# Details about the imaging run

## Run: 0000 1204 9003

* *sample used*: cRCRF1047T
* *date*: 28/01/2019
* *number of vials:* 2 jumbo vials, 2*10e8 cells
* *drug perturbation:* performed
* *imaging protocol:* high_intensity_6h

Regular 2/3 layout with pure and mixed population. 
**Layout Error:** plate was in the wrong order during imaging - with 180 degrees - CCLF barcode facing to wall because it was moved. This was 100% the case for imaging runs on the 31-1-19 and 01-02-19. This is most probably true for the runs on the days before.


## Run: 0000 1204 8903

* *sample used*: cRCRF1047T
* *date*: 05/02/2019
* *number of vials:* 2 jumbo vials, 2*10e8 cells
* *drug perturbation:* performed
* *imaging protocol:* high_intensity_6h

Regular 2/3 layout with pure and mixed population. 
No layout error during microscopy this time.

## Run: 0000 1204 8703

* *sample used*: cRCRF1066T
* *date*: 27/02/2019
* *number of vials:* 2 jumbo vials, 2*10e8 cells
* *drug perturbation:* performed
* *imaging protocol:* high_intensity_6h

Only limited cells during sample processing after depletion. I had to seed cells with limited density at 125k/ml (50% of usual concentration) for section 1 and seeded 100% pure raw cells in section 2. Section 3 remained empty. There were selected pipetting errors in wells $c("K16", "M16", "N13", "N14", "N15", "N16", "P13", "P14", "P15", "P16")$ that have to be excluded later on. 

# Data access

I access the RDS database. There are two tables that describe the data on the measurement and observation level.

```{r}
host_db = "biosensor.c9k2hfiwt5mi.us-east-2.rds.amazonaws.com"
pool <- pool::dbPool(RPostgres::Postgres(),
                       host = host_db,
                       dbname = "biosensor",
                       port = 5432,
                       user = "biosensor",
                       password = readLines("~/password.txt"))

db_list_tables(pool)
measurement <- tbl(pool, "measurement")
observation <- tbl(pool, "observation")
```

I load data for files that are relevant in this pilot. 

```{r}
barcodes <- c(12048703,
12048903,
12049003,
12094903,
12095103,
12095203,
12105403,
12105803) %>% as.character() %>% 
  paste0("0000", .)

measurement <- measurement %>% collect() %>% filter(id_barcode %in% barcodes)

measurement %>%
  write_rds(here::here("data/measurement.Rds"))

# keeps breaking the instance, too much data. 
# measurement %>%
#   semi_join(observation %>% collect(), .) %>% write_rds(here("data/observation.Rds"))
```

# Treatment annotation

I also load the drug library annotation file (Be aware that the column "alias" in the Kekule registration is misleading. It does nor represent the actual abbreviation/ treatment identity).

```{r, message=FALSE}
# Reading the compound submission file but ignoring the manual ID
plate_anno <- read_delim(here::here("data/annotation/20181016105007_registered_7296_NRindtorff_registration_Kekule.txt"), "\t", escape_double = FALSE, trim_ws = TRUE) %>% 
  # Using the Broad ID
  select(broad_sample = Broad_external_Id, compound_name) %>% 
  # Reading the annotation of the source plate for DMSO 
  left_join(read_delim(here::here("data/annotation/S-C-7296-01-B40-002.txt"), 
      "\t", escape_double = FALSE, trim_ws = TRUE) %>% select(well_position, 
                                                              broad_sample, 
                                                              mmoles_per_liter) %>% 
        drop_na() %>% 
        # Reading the annotation of the source plate H20
        rbind(.,read_delim(here::here("data/annotation/S-C_7296_01_B40_003.txt"), 
      "\t", escape_double = FALSE, trim_ws = TRUE) %>% select(well_position, 
                                                              broad_sample, 
                                                              mmoles_per_liter) %>% drop_na()), ., 
      # Joining by broad_id
      by = "broad_sample") %>%
  
  # Reading the annotation of the robot worklist that links source plate to destination plate
  left_join(read_csv(here::here("data/annotation/cclf_ascites_1910_complete_worklist.csv")) %>% 
    select(destination_well = well, well_position = source_well)) %>% 
  # Adding 0.1% DMSO wells
  select(-well_position, well = destination_well) %>%
  left_join(tibble(well = platetools::num_to_well(1:384, plate = 384)), .) %>% 
  mutate(broad_sample = if_else(is.na(broad_sample), "DMSO", broad_sample), 
         compound_name = if_else(is.na(compound_name), "DMSO", compound_name),
         mmoles_per_liter = if_else(is.na(mmoles_per_liter), 0, mmoles_per_liter)) %>% 
  mutate(compound_name = if_else(compound_name == "Boretzomib", "Bortezomib", compound_name),
         compound_name = if_else(compound_name == "Pembrolizumab", "H20/Pembrolizumab", compound_name)) %>%
  # I add section information
  mutate(row = substr(well, 1, 1) %>% match(., LETTERS),
         col = substr(well, 2, 3) %>% as.numeric()) %>%
  mutate(section = case_when(col %in% c(1:4, 13:16) ~ 1,
                             col %in% c(5:8, 17:20) ~ 2,
                             col %in% c(9:12, 21:24) ~ 3))

# I write the library annotation file. 
write_csv(plate_anno, here::here("data/annotation/cclf_ascites_1910_human_readable.csv"))
```

I plot the complete library. 

```{r}
df <- plate_anno
raw_map(data = df$compound_name,
        well = df$well,
        plate = 384) +
    ggtitle("cclf_ascites_1910") +
    scale_fill_brewer(type = "qual", palette = 2) +
    theme_dark()
```

# Missing Data

In order to more realisitically understand the way cells behave over time, we need to annotate timepoints with a real world time equivalent. We do so using by using the estimated protocol time and inferring the time after the start of the experiment, so that well A01 at timepoint 1 has a different real world time that well P24 at timepoint 1.

Because we do not have access to the exact file creation dates, we take an easy way out in which we are extrapolating the well wise acquisition time for each imaging run. This is complicated by the fact, that not every plate was completely imaged and not every protocol was exactly the same. In the future it would be best to have a protocol to write the file creation date for a whole directory into a .txt file that can then be uploaded to AWS S3.

I pull the plate layouts for every processed plate. 

```{r}
df <- measurement %>% 
  unite(run, measurement_no, iteration_no, remove = FALSE) %>%
  select(run, id_barcode, measurement_no,well) %>% 
  mutate(row = substr(well, 1, 1) %>% match(., LETTERS[16:1]),
         col = substr(well, 2, 3) %>% as.numeric()) %>% 
  mutate(status = 1)

df %>%
  ggplot(aes(col, row, fill = row)) + 
  geom_tile() + 
  scale_fill_viridis_c() +
  facet_grid(run~id_barcode) + 
  ggtitle("Imaged wells during pilot phase, not-corrected") + 
  theme_classic() #+
  #theme(axis.text.x = element_blank()) + 
  #theme(axis.text.y = element_blank())
```

I need to manually change the following: 
* rename imaging runs for *000012048703*
* rename order of imaging runs for *000012049003*

Also, I recognize that some data is missing. There are 3 possible explainations
* Data was never intended to be measured
* Data was accidently not measured
* Data was not uploaded
* Data was not indexed
* Data was not processed
* There were no observations for a given measurement

I am running two system commands to track sources of missing data. I list all files in our inbox directory and the flatfield directory. These are big files.

```{r, eval = FALSE}
system("aws s3 ls s3://ascstore/flatfield/ --recursive | awk '/IdentifySecondaryObjects.csv/' > ~/rapid/result_filelist.txt")
system("aws s3 ls s3://ascstore/metadata/ --recursive | awk '/.json/' > ~/rapid/metadata_filelist.txt")
system("aws s3 ls s3://ascstore/inbox/ --recursive > ~/rapid/inbox_filelist.txt")
```

I perform a double check and filter both results, metadata and raw data in the inbox.

```{r}
missing_result_files <- read_table2(here::here("result_filelist.txt"), 
    col_names = FALSE) %>% magrittr::set_colnames(c("date", "time", "size", "path")) %>% 
  mutate(id_observation = str_extract(path, pattern = "0000\\d+__\\d+-\\d\\d-\\d+T\\d+_\\d+_\\d+-Measurement_\\d-sk\\d-...-f..-ch\\d")) %>% 
  #mutate(size_filter = if_else(size < quantile(.99, size, na.rm = TRUE))) %>%
  anti_join(measurement %>% select(id_observation)) %>% 
  filter(size < 1000000) #>99% of all filesizes generally observed

nrow(missing_result_files)
```

No result files that have been generated are missing in our measurement overview.
Next, I check the available metadata for completeness.


```{r}
missing_metadata_files <- read_table2(here::here("metadata_filelist.txt"), 
    col_names = FALSE) %>% magrittr::set_colnames(c("date", "time", "size", "path")) %>% 
  mutate(id_measurement = str_extract(path, pattern = "0000\\d+__\\d+-\\d\\d-\\d+T\\d+_\\d+_\\d+-Measurement_\\d")) %>%
  mutate(id_barcode = str_extract(path, pattern = "0000\\d+"),
         channel = str_extract(path, pattern = "_[a-z][a-z]_") %>% substr(2,3),
         well = str_extract(path, pattern = "[A-Z]\\d+.json") %>% substr(1,3)) %>% 
  # only keeping barcodes I am interested in
  filter(id_barcode %in% barcodes) %>%
  #count(id_measurement, well) %>% 
  anti_join(measurement %>% select(id_measurement, well, id_barcode) %>% distinct()) %>% 
  left_join(measurement %>% select(id_measurement, contains("no")) %>% distinct())

missing_metadata_files %>% arrange(id_measurement, well, channel) %>% 
  select(id_measurement, well, channel) %>% 
  count(channel)
```

It turns out that missing measurements jobs only exist for 3 of the 4 job .json files available. The phase contrast jobs were mostly processed and generated a report! I believe this is due to the systems function, but will have to check in detail.

```{r, eval = FALSE}
inbox_files <- read_table2(here::here("inbox_filelist.txt"), 
    col_names = FALSE) %>% magrittr::set_colnames(c("date", "time", "size", "path")) %>% 
  mutate(id_measurement = str_extract(path, pattern = "0000\\d+__\\d+-\\d\\d-\\d+T\\d+_\\d+_\\d+-Measurement_\\d")) %>% 
  mutate(row = str_extract(path, pattern = "r\\d+") %>% substr(2,3) %>% as.numeric() %>% LETTERS[.],
         col = str_extract(path, pattern = "c\\d+") %>% substr(2,3),
         well = paste0(row, col),
         channel = str_extract(path, pattern = "ch\\d")) %>% 
  filter(well != "NANA")

missing_inbox_files <- inbox_files %>% 
  distinct(id_measurement, well, channel) %>% arrange(id_measurement, well) %>%
  # extracting the barcode
  mutate(id_barcode = str_extract(id_measurement, pattern = "0000\\d+")) %>% 
  # only keeping barcodes I am interested in
  filter(id_barcode %in% barcodes) %>%
  anti_join(measurement %>% select(id_measurement, well, id_barcode) %>% distinct()) %>%
  # adding some handy metadata for plotting
  left_join(measurement %>% select(id_measurement, id_barcode, contains("no")) %>% distinct()) 

saveRDS(missing_inbox_files, here::here("data/missing_inbox_files.Rds"))
```


```{r}
missing_inbox_files <- readRDS(here::here("data/missing_inbox_files.Rds"))
missing_inbox_files %>% arrange(id_measurement, well, channel) %>% 
  select(id_measurement, well, channel) %>% 
  count(channel)
```

I plot the files on both the inbox and metadata level that have not been processed yet. Note, however, that a larger portion of expected files simply does not exist on AWS S3! 

```{r}
df <- missing_inbox_files %>% 
  mutate(source = "inbox") %>%
  rbind(missing_metadata_files %>% select(id_measurement:iteration_no) %>% mutate(source = "metadata")) %>%
  unite(run, measurement_no, iteration_no, remove = FALSE) %>%
  count(run, id_barcode, measurement_no,well, source) %>% 
  mutate(row = substr(well, 1, 1) %>% match(., LETTERS[26:1]),
         col = substr(well, 2, 3) %>% as.numeric()) %>% 
  filter(!(source == "inbox" & n != 4))

df %>%
  #filter(source == "inbox") %>%
  ggplot(aes(col, row, fill = source)) + 
  geom_tile(alpha = 0.7) + 
  scale_fill_brewer(type = "qual") +
  facet_grid(run~id_barcode) + 
  ggtitle("Missing wells that have been imaged during pilot phase") + 
  theme_classic() +
  theme(axis.text.x = element_blank()) + 
  theme(axis.text.y = element_blank()) + 
  theme(legend.position = "bottom") + 
  labs(fill = "Number of channels available in inbox")
```

Based on this analysis, I conclude: 
* Most missing images for 000012094903 were not completely uploaded, they are simply missing. 
* Some (visible in the plot above) missing images for 000012094903 were uploaded, but never registered during metadata generation. This might be an issue with s3fs. 
* Missing images for 000012048903 were uploaded, exist in the inbox, are complete and were registered but never processed. This might be an issue with the cluster at some point.

In order to collect data for these missing wells, I will need to re-generate metadata for selected plates and resubmit processing jobs. I focus on 000012048903 as this is the biggest possible gain vs. input relationship. 

# Data Filtering

I extract the starting time for each plate using the exact timestamp. 

```{r}
measurement <- measurement %>% 
  # I drop NA entries, they are corrupted
  drop_na() %>%
  mutate(measurement_no = if_else(id_barcode == "000012048703" & measurement_no == 4, 2, measurement_no)) %>% 
  mutate(measurement_no = if_else(id_barcode == "000012048703" & measurement_no == 6, 3, measurement_no)) %>% 
  # extracting date
  mutate(date = str_extract(string = id_measurement, pattern = "20\\d\\d-\\d\\d-\\d\\dT\\d\\d_\\d\\d_\\d\\d") %>% 
           str_replace(pattern = "T", replacement = "-") %>% lubridate::ymd_hms())
  
```

Not every sample has 3 measurments on subsequent days. It will be hard to compare them. For now I build the analysis in a way that is only focused on the wall-clock imaging time.

```{r}
measurement %>% 
  select(id_barcode, date, measurement_no) %>% distinct() %>% arrange(date)
```

I fix some measurment_no indicators.

```{r}
measurement = measurement %>% 
  mutate(measurement_no = case_when(id_barcode == "000012049003" & date == ymd_hms("2019-01-28 21:02:07") ~ 1,
                                    id_barcode == "000012049003" & date == ymd_hms("2019-01-29 19:11:28") ~ 2,
                                    id_barcode == "000012049003" & date == ymd_hms("2019-01-31 18:40:09") ~ 3,
                                    id_barcode == "000012049003" & date == ymd_hms("2019-02-01 12:09:07") ~ 4,
                                    TRUE ~ measurement_no))
```

I plot the plate layout again.

```{r}
df <- measurement %>% 
  unite(run, measurement_no, iteration_no, remove = FALSE) %>%
  select(run, id_barcode, measurement_no,well) %>% 
  mutate(row = substr(well, 1, 1) %>% match(., LETTERS[26:1]),
         col = substr(well, 2, 3) %>% as.numeric()) %>% 
  mutate(status = 1)

df %>%
  ggplot(aes(col, row)) + 
  geom_tile(color = "black") + 
  facet_grid(run~id_barcode) + 
  ggtitle("Imaged wells during pilot phase, corrected") + 
  theme_classic() +
  theme(axis.text.x = element_blank()) + 
  theme(axis.text.y = element_blank())
```

# Time imputations

I impute time based on the following assumptions: 
* the measurement timepoint in the barcode reflects the time when a measurement was started
* there is a constant time that is needed for well completion
* each measurement starts at well P01 and ends at well A24
* in some cases, measurement protocols have implemented a waiting time between each imaging round (multiple imaging rounds per measurement with a t>0s gap between the rounds. I don't think this is a good idea, but it has been done.) 

I manually annotate the imaging protocols for each plate in our dataset.

```{r}
barcode_anno <- tibble(id_barcode = barcodes[1:3],
       library = "XXXX_2018/11/06",
       protocol = "ascites_mush_40x_mito_fitc_singlecell_z-2_lowerexp_fullplate_6h",
       confocal = FALSE,
       pause = FALSE) %>% # pause indicates wether or not the microscope took a forced break during acquisition. I don't think this is a good thing, but we introduced it when the confocal protocol turned out to be faster than others. We will get rid of it in the future. 
rbind(tibble(id_barcode = barcodes[4:6],
       library = "9375_2019/04/03",
       protocol = "ascites_mush_40x_mito_fitc_singlecell_z-2_lowerexp_fullplate_6h_with_break",
       confocal = TRUE,
       pause = TRUE)) %>% 
  cbind(sample = c("CCLF_cRCRF1066T",
                 "CCLF_cRCRF1047T",
                 "CCLF_cRCRF1047T",
                 "CCLF_cRCRF1047T",
                 "CCLF_cRCRF1050T",
                 "CCLF_cRCRF1068T")) %>% 
  left_join(read_csv(here::here("data/CCLF Ascites Samples - biosensor_cohort.csv")) %>% 
  select(sample, patient_id))


```

I now create a manual well-wise annotation file for every barcode & measurement using the plate annotation as the basis.

```{r}
anno <- measurement %>% select(contains("id"), well, date, contains("_no")) %>% 
  select(-contains("observation")) %>% 
  distinct() %>% 
  left_join(plate_anno) %>% 
  left_join(barcode_anno) %>%
    mutate(row = substr(well, 1, 1) %>% match(., LETTERS),
         col = substr(well, 2, 3) %>% as.numeric()) %>%
  # joining anno file and formatting annotation according to the written/online documentation
  mutate(content = case_when(id_barcode == "000012048703" & section == 1 ~ "CD45-",
                            id_barcode == "000012048703" & section == 2 ~ "raw",
                            id_barcode == "000012048703" & section == 2 ~ "empty",
                            # next plate
                            id_barcode == "000012048903" & section %in% c(1,2) ~ "CD45-",
                            id_barcode == "000012048903" & section == 3 ~ "CD45-_raw_1:1",
                            # next plate
                            id_barcode == "000012049003" & section %in% c(1,2) ~ "CD45-",
                            id_barcode == "000012049003" & section == 3 ~ "CD45-_raw_1:1",
                            # next plate, processing Mushriq's - need confirmation
                            id_barcode == "000012094903" & section %in% c(1,2) ~ "CD45-",
                            id_barcode == "000012094903" & section == 3 ~ "empty",
                            # next plate
                            id_barcode == "000012095103" & section %in% c(1,2) ~ "CD45-",
                            id_barcode == "000012095103" & section == 3 ~ "empty",
                            # next plate - needs confirmation by Mushriq
                            id_barcode == "000012095203" & section %in% c(1,2, 3) ~ "CD45-"),
          flag = case_when(id_barcode == "000012048703" & well %in% c("K16", "M16", "N13", "N14", 
                                                                     "N15", "N16", "P13", "P14", "P15", "P16") ~ TRUE,
                          
                          TRUE ~ FALSE)) %>% 
  # changing the library H20/Pembolizumab acc. to the library status. 
  mutate(compound_name = if_else(compound_name == "H20/Pembrolizumab", "H20", compound_name)) %>%
  
  # Now I also add the imaging times. What matters, is the time per well that is spend per protocol. We can estimate that number based on the hand-written protocols and logs from the Opera microscope.
  mutate(time_per_plate = case_when(confocal == FALSE ~ hms("6:37:23") %>% lubridate::seconds() %>% as.numeric(),
                                    confocal == TRUE ~ hms("4:49:23") %>% lubridate::seconds() %>% as.numeric()),
         time_per_well = time_per_plate/384)
```

I will add this table to the database later on. I visualize the database layout.

```{r, eval = FALSE}
s_query <- datamodelr::dm_re_query("postgres")
dm_biosensor <- dbGetQuery(pool, s_query) 

dm_biosensor <- datamodelr::as.data_model(dm_biosensor)

graph <- datamodelr::dm_create_graph(dm_biosensor, rankdir = "RL")
datamodelr::dm_render_graph(graph)
```

Now I estimate the exact imaging time for each well based on the *time_per_well* feature and the device-specific order of imaging.
First, I define the a layout of the device-specific imaging order.

```{r}
# I define a layout of the device-specific order of imaging.
measurement_layout <- tibble(row = rep(c(LETTERS[c(16:1)], LETTERS[c(1:16)]), times = 12),
       col = rep(c(1:24), each = 16)) %>% 
  mutate(col = str_pad(col, 2, side = "left", pad = "0"),
         well = paste0(row, col),
         index = 1:384) %>% 
  dplyr::select(well, index)

df <- measurement_layout

raw_map(data = df$index,
        well = df$well,
        plate = 384) +
    ggtitle("Time layout for imaging protocol") +
    theme_dark() +
    scale_fill_viridis()
```

Now I combine the layout with the *time_per_well* estimates. 

```{r}
anno_w_time <- anno %>% 
  left_join(measurement_layout , by = "well") %>% 
  arrange(id_barcode, measurement_no, iteration_no, index) %>% 
  group_by(id_barcode, measurement_no) %>%
  mutate(time = cumsum(time_per_well)) %>% 
  arrange(id_barcode, measurement_no, iteration_no, index) %>% 
  # adding a the break time for some plates
  mutate(time = if_else(iteration_no == 2 & pause == TRUE, time + (6*3600-time_per_plate), time)) %>% 
  mutate(date = date + seconds(time)) %>%
  # overwritinig time to be the relative absolute number of hours since the very first well. 
  group_by(id_barcode) %>% 
  mutate(time = date - min(date),
         time = as.numeric(time),
         time = time/3600) # converting to hours. 
  
```

I can give a historic overview about the CCLF imaging activity.

```{r}
anno_w_time %>% 
  group_by(id_barcode, sample, library, patient_id) %>% 
  summarise(date = min(date))  %>% 
  ggplot(aes(date, sample, color = patient_id, label = id_barcode)) + 
  geom_point(size = 3) + 
  ggrepel::geom_text_repel(color = "black") +
  theme_classic() + 
  theme(legend.position = "bottom") + 
  scale_color_brewer(type = "qual") + 
  ggtitle("Samples and their time of acquisition") + 
  labs(color = "Patient")
```


I plot the time layout for every plate. 

```{r}
df <- anno_w_time %>% 
  unite(run, measurement_no, iteration_no, remove = FALSE) %>%
  select(run, id_barcode, measurement_no,well, time) %>% 
  mutate(row = substr(well, 1, 1) %>% match(., LETTERS[26:1]),
         col = substr(well, 2, 3) %>% as.numeric()) %>% 
  mutate(status = 1)

df %>%
  ggplot(aes(col, row, fill = time)) + 
  geom_tile() + 
  facet_grid(run~id_barcode) + 
  scale_fill_viridis_c() +
  ggtitle("Imaged wells during pilot phase, corrected") + 
  theme_classic() +
  theme(axis.text.x = element_blank()) + 
  theme(axis.text.y = element_blank(),
        legend.position = "bottom") + 
  labs(fill = "Time in hours")
```

This approach to time inference has obvious weaknesses: 
* The method can not distinguish between wells that were purposefully omitted during image capture or failures during the image analysis workflow leading to missing data. While in the 1st scenario, the model is accurate, the second scenario leads to a systematic underestimation of actual acquisition time for every well thereafter.

I write the annotation to the database in a separate table.

```{r, eval = FALSE}
# set arguments carefully.
DBI::dbWriteTable(pool, "annotation", anno_w_time, overwrite = FALSE, append = FALSE)
# annotation <- tbl(pool, "annotation")
```

# EDA and QC

The object segmentation is currently not ideal. Therefore I remove objects that are clearly over or under-segmented. I evaluate the size distribution of segmented objects and exclude objects that have extreme values. 

I filter all extremely sized objects from the database.

```{r, eval = FALSE}
# show behaviour of viability over time for untreated and control samples
# show robustness of sample viability behaviour over time between replicates
# show similar patterns in compound response for both biological replciates 

percentiles <- dbGetQuery(pool, 'select
  percentile_cont(0.01) within group (order by observation.area_shape_area),
  percentile_cont(0.99) within group (order by observation.area_shape_area)
from observation;')

saveRDS(percentiles, here::here("data/percentiles.Rds"))
```


```{r}
percentiles <- readRDS(here::here("data/percentiles.Rds"))
max_area <- max(percentiles[1,])
min_area <- min(percentiles[1,])


# updating the database
observation_f <- observation %>%
  filter(area_shape_area < max_area & area_shape_area > min_area) %>%
  left_join(measurement %>% select(contains('id'), well), copy = TRUE) %>%
  left_join(anno_w_time,copy= TRUE) %>%
  filter(flag == FALSE & content == "CD45-") %>%
  filter(measurement_no < 4) %>%
  mutate(run = paste0(measurement_no, "_", iteration_no))


```

I am also creating a local realization based on these cutoff values. This is a subsample of the dataset.

```{r, eval = FALSE}
# observation_sample <- dbGetQuery(pool, 
# "SELECT * 
# FROM observation O
# WHERE O.id_observation IN (SELECT id_observation 
# FROM annotation A
# WHERE A.confocal = 'TRUE' AND A.compound_name = 'DMSO')
# ORDER BY RANDOM() 
# LIMIT 3000;")

observation_f_real <- observation_f %>% 
  filter((compound_name == 'DMSO' | compound_name == 'Bortezomib')) %>% 
  # filter(confocal == TRUE) %>% 
   collect()

observation_f_sample <- observation_f_real %>% 
  select(contains("_ch3"), contains("_ch4"), 
         #contains("area"),
         id_measurement, id_barcode, time, compound_name) %>% 
  group_by(id_measurement, compound_name) %>% 
  sample_n(2000) %>%
  ungroup() 

observation_f_umap_in <- observation_f_sample %>%
  select(-contains("location"), -contains("_center_"), -id_measurement, -id_barcode,
         -contains("number"), -time, -compound_name) 

observation_f_umap_out <- uwot::umap(observation_f_umap_in, pca = 50, verbose = TRUE) %>%
  as_tibble() %>%
  clean_names()

observation_f_umap_out %>%
  cbind(observation_f_sample) %>%
  mutate(ctrl = if_else(compound_name == "Bortezomib", TRUE, FALSE)) %>%
 ggplot(aes(v1, v2, color = time)) + # area_shape_area intensity_integrated_intensity_ch3
  geom_point(alpha = 0.1) + 
  facet_grid(compound_name ~ id_barcode) +
  scale_color_viridis_c() + 
  theme_bw() + 
  labs(title = "UMAP embedding of DMSO and Bortezomib treated cells across cohort",
       x = "UMAP 1", 
       y = "UMAP 2",
       color = "Time [h]") + 
  ggsave(here::here("umap_borte_dmso.png"), width = 7, height = 3.5)
  
```


```{r, eval = FALSE}
gg_size_histogramm <- observation_f %>% 
  filter(section == 1 & compound_name == "DMSO") %>% # A random DMSO treated well
  select(area_shape_area, run, id_barcode) %>%
  collect() %>%
  ggplot(aes(area_shape_area)) + 
  geom_histogram(alpha = 0.3, position="identity") + 
  theme_classic() 

gg_size_histogramm + 
  scale_fill_brewer(type = "qual") +
  facet_grid(run~id_barcode) + 
  labs(title = "Size Distribution of segmented objects",
       subtitle = "after removing 1% and 99% percentile",
       caption = "")
```

I am wondering if the number of objects stays the same over time and across wells. I calculate QC threshold values for each batch that remove wells, if the number of objects is unexpectedly large.

```{r, eval = FALSE}
get_cell_count <- function(db){
  cell_count <- list()
  
  cell_count$distribution <- db %>% 
    count(well, run, id_barcode, compound_name) %>% 
    collect()
  
  cell_count$threshold <- cell_count$distribution %>%
  ungroup() %>%
  group_by(run, id_barcode) %>% 
  summarise(sd = sd(n),
            mean = mean(n),
            min = mean-3*sd,
            max = mean+3*sd
            )
  
  return(cell_count)
}

cell_count <- observation_f %>% get_cell_count()


cell_count$distribution %>%
  ungroup() %>%
  mutate(well = factor(well) %>% fct_inorder(),
         n = as.numeric(n)) %>%
  ggplot(aes(well, n )) + 
  geom_point(alpha = 0.8) + 
  #geom_smooth(color = "red") +
  geom_hline(data = cell_count$threshold, aes(yintercept = min), linetype = "dashed") + 
  geom_hline(data = cell_count$threshold, aes(yintercept = max), linetype = "dashed") + 
  facet_grid(id_barcode ~ run) + 
  theme(axis.text.x = element_blank(),
        legend.position = "None") + 
  scale_color_brewer() +
  labs(title = "Number of objects and their distribution across the plate",
       subtitle = "shown are well A01 to P24",
       caption = "") 
```



```{r}
dye_intensity <- observation_f %>% 
  dplyr::select(intensity_integrated_intensity_ch3, intensity_integrated_intensity_ch4, 
                id_barcode, id_measurement, id_observation ,run,
                time, well, compound_name,
                object_number, image_number) %>% 
  collect()

dye_intensity <- dye_intensity %>% 
  mutate(intensity_integrated_intensity_ch3_log = log(intensity_integrated_intensity_ch3)) %>% 
  mutate(intensity_integrated_intensity_ch4_log = log(intensity_integrated_intensity_ch4)) %>% 
  group_by(id_barcode, run) %>% 
  mutate(intensity_integrated_intensity_ch3_log_scale = scale(intensity_integrated_intensity_ch3_log)) %>% 
  mutate(intensity_integrated_intensity_ch4_log_scale = scale(intensity_integrated_intensity_ch4_log)) %>% 
  ungroup()

dye_cluster <- dye_intensity %>% 
  select(intensity_integrated_intensity_ch4, # also test without log_scale in a bit
         intensity_integrated_intensity_ch3,
         id_barcode, run) %>%
  nest(contains('integrated')) %>% 
  #head(6) %>%
  #mutate(kmeans = purrr::map(data, ~ .x %>% as.matrix %>% kmeans(centers = 2)),
  mutate(kmeans = purrr::map(data, ~ .x %>% as.matrix %>% mclust::Mclust(., G = 2)),
         cluster = purrr::map(kmeans, ~ .x %>% .$classification),
         uncertainty = purrr::map(kmeans, ~ .x %>% .$uncertainty)) %>% 
  unnest(data, cluster, uncertainty)

# Reordering cluster labels, so the cluster with the lower average ch4 intensity is labeled as cluster == "dead"


dye_overview <- dye_cluster %>% 
  group_by(id_barcode, run, cluster) %>% 
  summarise(ch4_mean = mean(intensity_integrated_intensity_ch4)) %>% 
  arrange(id_barcode, run, ch4_mean) %>% 
  cbind(., new_cluster = rep(c(1,2), times = nrow(dye_cluster %>% 
                                              count(id_barcode, run)))) %>% 
  left_join(dye_cluster, .) %>% 
  select(-cluster, -ch4_mean) %>%
  rename(cluster = new_cluster) %>% 
  mutate(cluster = if_else(cluster == 1, "dead", "alive") %>% factor) %>% 
  cbind(., dye_intensity %>% 
                        select(-run, -id_barcode, 
                               - contains('intensity_integrated_intensity_ch3'), 
                               - contains('intensity_integrated_intensity_ch4')))

# dye_overview %>%
#   mutate(cluster = if_else(cluster == 1, "viable", "dead")) %>%
#   ggplot(aes(intensity_integrated_intensity_ch4_log, fill = cluster)) + 
#   geom_histogram(position = "identity") +
#   theme_bw() + 
#   facet_grid(run ~ id_barcode) + 
#   scale_x_log10() 

dye_overview %>% write_rds(here::here("data/dye_overview.Rds"))
```

```{r}
dye_overview  %>% 
  sample_n(5000) %>%
  mutate(cluster = if_else(cluster == 1, "viable", "dead")) %>%
  ggplot(aes(intensity_integrated_intensity_ch4, intensity_integrated_intensity_ch3)) + 
  geom_hex() +
  theme_bw() + 
  facet_grid(run ~ id_barcode) + 
  scale_x_log10() + 
  scale_y_log10() + 
  scale_fill_viridis_c() +
  geom_density_2d(aes(color = cluster)) + 
  scale_color_brewer(type = "qual")
```


```{r}
dye_overview %>% 
  filter(id_barcode == "000012095103") %>%
  filter(compound_name == "DMSO") %>%
  #sample_n(1000000) %>%
  #select(intensity_integrated_intensity_ch4, intensity_integrated_intensity_ch3, well, time, plate) %>% 
  ggplot(aes(time, intensity_integrated_intensity_ch3, color = id_barcode)) + 
  
  geom_hex() +
  geom_smooth() +
  theme_bw() + 
  #facet_grid( ~ id_barcode) + 
  #scale_x_log10() + 
  scale_y_log10() + 
  scale_fill_viridis()
```



# Quick and Dirty cutoff 

```{r}
 %>% 
  group_by(well, time, plate) %>%
  summarise(n = n(),
            sum = sum(alive)) %>% 
  mutate(prop = sum/n)

viability <- replicate_1_jdr %>% 
  #head(100) %>% 
  collect()
```

```{r}
viability %>% 
  ggplot(aes(prop, n)) + 
  geom_smooth(method = "lm") + 
  geom_point(alpha = 0.3) +
  theme_bw() + 
  facet_grid(plate ~ time) + 
  scale_y_log10()
  
```

```{r}
viability_anno <- viability %>% 
  left_join(plate_anno) %>% 
  ungroup() #%>%
 # group_by(compound_name, concentration, time_point) %>% 
 # summarise(n = sum(n),
     #       sum = sum(sum),
    #        prop = sum/n)
```

```{r}
viability_anno %>%
  #filter(compound_name == "Trametinib") %>% 
  ggplot(aes(time_point, prop, color = concentration, group = concentration)) + 
  geom_point() + 
  geom_line(aes(group = concentration)) +
  theme_bw() + 
  facet_wrap(~compound_name)
```


```{r}
viability_anno %>%
  filter(compound_name %in% c("DMSO", "Boretzomib")) %>% 
  ggplot(aes(time_point, prop, color = compound_name, group = concentration)) + 
  geom_point() + 
  geom_smooth(aes(group = compound_name))+
 #geom_line(aes(group = compound_name)) +
  theme_bw() #+ 
  #facet_wrap(~compound_name)
```

```{r}
viability_anno %>%
  #filter(compound_name == "Afatinib") %>% 
  ggplot(aes(concentration, prop, color = time_point, group = time_point)) + 
  geom_point() + 
  geom_line(aes(group = time_point)) +
  theme_bw() + 
  facet_wrap(~compound_name) + 
  scale_x_reverse()
```


We can see a slight shift of estimated proportions dependent on the day of measurement and the number of observations. We should apply an empirical Bayes estimator to account for both effects that impact proportions.

# UMAP

I define UMAP parameters for efficient clustering 

```{r}
umap_settings_cluster <- umap.defaults
# according to https://umap-learn.readthedocs.io/en/latest/clustering.html we change our parameters
umap_settings_cluster$n_neighbors <- 30
umap_settings_cluster$min_dist <- 0
```

I create a sample from the dataset

```{r}
set.seed(3423)

sample_index <- index_1 %>% collect() %>% 
  group_by(well, time, plate) %>% 
  sample_n(10) %>% 
  ungroup() %>%
  dplyr::select(id, object_number, image_number)


sample <- replicate_1_j %>% 
  #removing outliers
  dplyr::filter(area_shape_area < max & area_shape_area > min) %>% 
  left_join(sample_index,., copy = TRUE) %>% 
  drop_na()
```



I create a UMAP embedding

```{r}
no_umap_sample <- sample %>% 
  dplyr::select(everything(),
                -contains("location"),
                -contains("_x"),
                -contains("_y"),
                -id, -plate, -time, -well, -field, -channel,
                -number_object_number,
                -parent_identify_primary_objects,
                -image_number,
                -object_number,
                -area_shape_euler_number,
                -area_shape_center_z) %>% 
  as.data.frame() %>%
  #mutate_all(funs(as.numeric)) %>%
  as.matrix()

no_umap_sample_bf <- no_umap_sample %>% 
  as.data.frame() %>%
  dplyr::select(everything(), 
                -contains("ch3"),
                -contains("ch4")) %>% 
  mutate_all(funs(as.numeric)) %>%
  as.matrix()

umap_sample <- no_umap_sample %>%
  umap(config = umap_settings_cluster)

saveRDS(umap_sample, "umap_sample.Rdata")

umap_sample_bf <- no_umap_sample_bf %>%
  umap(config = umap_settings_cluster)

saveRDS(umap_sample_bf, "umap_sample_bf.Rdata")
```

Do I need a UMAP embedding? 

```{r, eval = FALSE}
cluster_no_umap <- no_umap_sample %>% 
  dbscan::hdbscan(minPts = 200)

saveRDS(cluster_no_umap, "cluster_no_umap.Rdata")
```

After running the embedding, we see how most cells can actually not be assigned to a given cluster. Let's try applying a UMAP nonlinearity.

```{r}
readRDS("cluster_no_umap.Rdata")
```

I run a quick QC of the embedding. 

```{r}
umap_sample$layout %>% as_tibble() %>% cbind(sample,.) %>%
  ggplot(aes(V1, V2, color = intensity_integrated_intensity_ch3)) + 
  geom_point(alpha = 0.3) + 
  facet_grid(plate ~ time) + 
  theme_bw() + 
  scale_color_viridis()
  
  
cluster <- umap_sample$layout %>% 
  dbscan::hdbscan(minPts = 200)

umap_sample$layout %>% as_tibble() %>% cbind(cluster$cluster,.) %>%
  magrittr::set_colnames(c("cluster", "V1", "V2")) %>%
  mutate(cluster = factor(cluster)) %>%
  cbind(sample,.) %>%
  ggplot(aes(V1, V2, color = cluster)) + 
  geom_point(alpha = 0.3) + 
  facet_grid(plate ~ time) + 
  theme_bw()
```


```{r}
umap.learn.predict
```


I also create an embedding only based on brightfield data 


```{r}
umap_sample_bf$layout %>% as_tibble() %>% cbind(sample,.) %>%
  ggplot(aes(V1, V2, color = intensity_integrated_intensity_ch4)) + 
  geom_point(alpha = 0.3) + 
  facet_grid(plate ~ time) + 
  theme_bw() + 
  scale_color_viridis()
  
  
cluster_bf <- umap_sample_bf$layout %>% 
  dbscan::hdbscan(minPts = 200)

umap_sample_bf$layout %>% as_tibble() %>% cbind(cluster_bf$cluster,.) %>%
  magrittr::set_colnames(c("cluster", "V1", "V2")) %>%
  mutate(cluster = factor(cluster)) %>%
  cbind(sample,.) %>%
  ggplot(aes(V1, V2, color = cluster)) + 
  geom_point(alpha = 0.3) + 
  facet_grid(plate ~ time) + 
  theme_bw()
```

I decide to continue my analysis with only brightfield images. It turns out that the features from brightfield images carry a lot of information and -in addition- are more stable over time. Cell Event and TMRM are not stable over time.

On the other hand, we don't know if the brightfield derived features are just not expressive enough and the reduction of Cell Event is actually driven by biology, rather than simple bleaching or degradation over time. 

As a consequence, it might be best to generate both projections for the dataset.

```{r}
id_of_interest = "000012048903__2019-02-06T20_33_15-Measurement_2-sk1-A01-f01-ch1"

transfer_umap <- function(id_of_interest, bf_umap_object, umap_object){
# collect data
umap_df <- replicate_1_j %>% 
  dplyr::filter(area_shape_area < max & area_shape_area > min) %>% 
  filter(id %in% id_of_interest) %>%
  dplyr::select(everything(),
                -contains("location"),
                -contains("_x"),
                -contains("_y"),
                -(image_number:object_number),
                -number_object_number,
                -parent_identify_primary_objects,
                -(id:channel),
                -area_shape_euler_number,
                -area_shape_center_z) %>% collect() %>%
  distinct()

# matrix
umap_matrix <- umap_df %>%
  as.matrix()

# matrix brightfield
umap_matrix_bf <- umap_df %>% 
  dplyr::select(everything(), 
                -contains("ch3"),
                -contains("ch4")) %>% 
  as.matrix()

# umap brightfield
output_bf <- predict(bf_umap_object, umap_matrix_bf) %>% 
  janitor::clean_names() %>% 
  magrittr::set_colnames(paste0(colnames(.), "_bf"))

# umap
output <- predict.umap(umap_object, umap_matrix) %>% 
  janitor::clean_names()
  
}
  
```


```{r}
tmp <- replicate_1_j %>% 
  dplyr::filter(area_shape_area < max & area_shape_area > min) %>%
  dplyr::filter(time == "sk1", plate == "000012048903__2019_02_05T20_27_41_Measurement_1") %>% 
  dplyr::select(area_shape_area:texture_variance_ch4_3_03) %>% 
  collect()
```


# Evaluate Afatinib + DMSO UMAP embedding 


For the sake of this demo, I will overwrite my sample and only focus on cells that were treated with Afatinib. 

```{r}
afatinib_index <- index_1 %>% collect() %>% 
  left_join(plate_anno) %>% 
  mutate(time_point = stringr::str_sub(plate, -1, -1) %>% as.numeric(),
         time_point = time_point*2,
         time_point = if_else(time == "sk1", time_point-1, time_point)) %>% 
  filter(compound_name %in% c("Afatinib", "DMSO")) %>% 
  .$id

sample <- replicate_1_j %>% 
  #removing outliers
  dplyr::filter(area_shape_area < max & area_shape_area > min) %>% 
  dplyr::filter(id %in% afatinib_index) %>%
  collect()
```



```{r}
no_umap_sample <- sample %>% 
  dplyr::select(everything(),
                -contains("location"),
                -contains("_x"),
                -contains("_y"),
                -id, -plate, -time, -well, -field, -channel,
                -number_object_number,
                -parent_identify_primary_objects,
                -image_number,
                -object_number,
                -area_shape_euler_number,
                -area_shape_center_z) %>% 
  as.data.frame() %>%
  #mutate_all(funs(as.numeric)) %>%
  as.matrix()

no_umap_sample_bf <- no_umap_sample %>% 
  as.data.frame() %>%
  dplyr::select(everything(), 
                -contains("ch3"),
                -contains("ch4")) %>% 
  mutate_all(funs(as.numeric)) %>%
  as.matrix()

umap_sample_afa_dmso <- no_umap_sample_bf %>%
  umap(config = umap_settings_cluster)

saveRDS(umap_sample_afa_dmso, "umap_sample_dmso_afatinib.Rdata")
```


```{r}
umap_sample_afa_dmso$layout %>% as_tibble() %>% #cbind(cluster_bf$cluster,.) %>%
  magrittr::set_colnames(c("V1", "V2")) %>%
  #mutate(cluster = factor(cluster)) %>%
  cbind(sample %>% dplyr::select(well, time, plate, intensity_integrated_intensity_ch4),.) %>%
  left_join(plate_anno) %>% 
  mutate(time_point = stringr::str_sub(plate, -1, -1) %>% as.numeric(),
         time_point = time_point*2,
         time_point = if_else(time == "sk1", time_point-1, time_point)) %>% 
  ungroup() %>%
  ggplot(aes(V1, V2, color = intensity_integrated_intensity_ch4)) + 
  geom_point(alpha = 0.1) + 
  #geom_density_2d() +
  facet_grid(time_point ~ concentration) + 
  theme_bw() + 
  scale_color_viridis()
```



