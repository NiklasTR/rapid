---
title: "Phase 1 - Reproducibility"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)
```

Loading packages

```{r}
library(tidyverse)
library(reticulate)
library(lubridate)

#clustering
library(umap)
library(dbscan)

#plotting
library(patchwork)
library(janitor)
library(platetools)
library(viridis)

# database
library(RPostgres)
library(pool)
library(RSQLite)
library(DBI)

library(here)
library(googlesheets4)
```

# Introduction

This vignette comprises 6 drug screens that were performed in 2019. The data contains the following experiments: 
* 3 runs performed based on the same sample (cRCRF1047T by PT-2HF4C) on independent timepoints from cryopreserved stocks by two operators (2 by Niklas Rindtorff, 1 by Mushriq Al-Jazrawe. 
* 2 runs with samples from the same patient, wich were collected a few weeks after therapy initiation with a 5FU based therapy (CCLF_cRCRF1066T and CCLF_cRCRF1050T by PT-2HF4C).
* 1 run with a sample from a different patient (CCLF_cRCRF1068T by PT-2INIB).

# Goal

The goals of this analysis are: 
* show behaviour of viability over time for untreated and control samples
* show robustness of sample viability behaviour over time between replicates
* show similar patterns in compound response for both biological replciates 
* explore differences in compound response between samples

This analysis does not:
* explore alternative & in-depth ways to describe drug response, for example by predicting timepoint and drug-concentration from a set of single cells
* explore phenotype differences between immune cells and cancer cells in the fluid samples. This data is available for some of the data.

# Details about the imaging run

## Run: 0000 1204 9003

* *sample used*: cRCRF1047T
* *date*: 28/01/2019
* *number of vials:* 2 jumbo vials, 2*10e8 cells
* *drug perturbation:* performed
* *imaging protocol:* high_intensity_6h

Regular 2/3 layout with pure and mixed population. 
**Layout Error:** plate was in the wrong order during imaging - with 180 degrees - CCLF barcode facing to wall because it was moved. This was 100% the case for imaging runs on the 31-1-19 and 01-02-19. This is most probably true for the runs on the days before.


## Run: 0000 1204 8903

* *sample used*: cRCRF1047T
* *date*: 05/02/2019
* *number of vials:* 2 jumbo vials, 2*10e8 cells
* *drug perturbation:* performed
* *imaging protocol:* high_intensity_6h

Regular 2/3 layout with pure and mixed population. 
No layout error during microscopy this time.

## Run: 0000 1204 8703

* *sample used*: cRCRF1066T
* *date*: 27/02/2019
* *number of vials:* 2 jumbo vials, 2*10e8 cells
* *drug perturbation:* performed
* *imaging protocol:* high_intensity_6h

Only limited cells during sample processing after depletion. I had to seed cells with limited density at 125k/ml (50% of usual concentration) for section 1 and seeded 100% pure raw cells in section 2. Section 3 remained empty. There were selected pipetting errors in wells $c("K16", "M16", "N13", "N14", "N15", "N16", "P13", "P14", "P15", "P16")$ that have to be excluded later on. 

# Data access

I access the RDS database. There are two tables that describe the data on the measurement and observation level.

```{r}
host_db = "biosensor.c9k2hfiwt5mi.us-east-2.rds.amazonaws.com"
pool <- pool::dbPool(RPostgres::Postgres(),
                       host = host_db,
                       dbname = "biosensor",
                       port = 5432,
                       user = "biosensor",
                       password = readLines("~/password.txt"))

db_list_tables(pool)
measurement <- tbl(pool, "measurement")
observation <- tbl(pool, "observation")
```

I load data for files that are relevant in this pilot. 

```{r}
barcodes <- c(12048703,
12048903,
12049003,
12094903,
12095103,
12095203,
12105403,
12105803) %>% as.character() %>% 
  paste0("0000", .)

measurement <- measurement %>% collect() %>% filter(id_barcode %in% barcodes)

measurement %>%
  write_rds(here::here("data/measurement.Rds"))

# keeps breaking the instance, too much data. 
# measurement %>%
#   semi_join(observation %>% collect(), .) %>% write_rds(here("data/observation.Rds"))
```

# Treatment annotation

I also load the drug library annotation file (Be aware that the column "alias" in the Kekule registration is misleading. It does nor represent the actual abbreviation/ treatment identity).

```{r, message=FALSE}
# Reading the compound submission file but ignoring the manual ID
plate_anno <- read_delim(here::here("data/annotation/20181016105007_registered_7296_NRindtorff_registration_Kekule.txt"), "\t", escape_double = FALSE, trim_ws = TRUE) %>% 
  # Using the Broad ID
  select(broad_sample = Broad_external_Id, compound_name) %>% 
  # Reading the annotation of the source plate for DMSO 
  left_join(read_delim(here::here("data/annotation/S-C-7296-01-B40-002.txt"), 
      "\t", escape_double = FALSE, trim_ws = TRUE) %>% select(well_position, 
                                                              broad_sample, 
                                                              mmoles_per_liter) %>% 
        drop_na() %>% 
        # Reading the annotation of the source plate H20
        rbind(.,read_delim(here::here("data/annotation/S-C_7296_01_B40_003.txt"), 
      "\t", escape_double = FALSE, trim_ws = TRUE) %>% select(well_position, 
                                                              broad_sample, 
                                                              mmoles_per_liter) %>% drop_na()), ., 
      # Joining by broad_id
      by = "broad_sample") %>%
  
  # Reading the annotation of the robot worklist that links source plate to destination plate
  left_join(read_csv(here::here("data/annotation/cclf_ascites_1910_complete_worklist.csv")) %>% 
    select(destination_well = well, well_position = source_well)) %>% 
  # Adding 0.1% DMSO wells
  select(-well_position, well = destination_well) %>%
  left_join(tibble(well = platetools::num_to_well(1:384, plate = 384)), .) %>% 
  mutate(broad_sample = if_else(is.na(broad_sample), "DMSO", broad_sample), 
         compound_name = if_else(is.na(compound_name), "DMSO", compound_name),
         mmoles_per_liter = if_else(is.na(mmoles_per_liter), 0, mmoles_per_liter)) %>% 
  mutate(compound_name = if_else(compound_name == "Boretzomib", "Bortezomib", compound_name),
         compound_name = if_else(compound_name == "Pembrolizumab", "H20/Pembrolizumab", compound_name)) %>%
  # I add section information
  mutate(row = substr(well, 1, 1) %>% match(., LETTERS),
         col = substr(well, 2, 3) %>% as.numeric()) %>%
  mutate(section = case_when(col %in% c(1:4, 13:16) ~ 1,
                             col %in% c(5:8, 17:20) ~ 2,
                             col %in% c(9:12, 21:24) ~ 3))

# I write the library annotation file. 
write_csv(plate_anno, here::here("data/annotation/cclf_ascites_1910_human_readable.csv"))
```

I plot the complete library. 

```{r}
df <- plate_anno
raw_map(data = df$compound_name,
        well = df$well,
        plate = 384) +
    ggtitle("cclf_ascites_1910") +
    scale_fill_brewer(type = "qual", palette = 2) +
    theme_dark()
```

# Missing Data

In order to more realisitically understand the way cells behave over time, we need to annotate timepoints with a real world time equivalent. We do so using by using the estimated protocol time and inferring the time after the start of the experiment, so that well A01 at timepoint 1 has a different real world time that well P24 at timepoint 1.

Because we do not have access to the exact file creation dates, we take an easy way out in which we are extrapolating the well wise acquisition time for each imaging run. This is complicated by the fact, that not every plate was completely imaged and not every protocol was exactly the same. In the future it would be best to have a protocol to write the file creation date for a whole directory into a .txt file that can then be uploaded to AWS S3.

I pull the plate layouts for every processed plate. 

```{r}
df <- measurement %>% 
  unite(run, measurement_no, iteration_no, remove = FALSE) %>%
  select(run, id_barcode, measurement_no,well) %>% 
  mutate(row = substr(well, 1, 1) %>% match(., LETTERS[16:1]),
         col = substr(well, 2, 3) %>% as.numeric()) %>% 
  mutate(status = 1)

df %>%
  ggplot(aes(col, row, fill = row)) + 
  geom_tile() + 
  scale_fill_viridis_c() +
  facet_grid(run~id_barcode) + 
  ggtitle("Imaged wells during pilot phase, not-corrected") + 
  theme_classic() #+
  #theme(axis.text.x = element_blank()) + 
  #theme(axis.text.y = element_blank())
```

I need to manually change the following: 
* rename imaging runs for *000012048703*
* rename order of imaging runs for *000012049003*

Also, I recognize that some data is missing. There are 3 possible explainations
* Data was never intended to be measured
* Data was accidently not measured
* Data was not uploaded
* Data was not indexed
* Data was not processed
* There were no observations for a given measurement

I am running two system commands to track sources of missing data. I list all files in our inbox directory and the flatfield directory. These are big files.

```{r, eval = FALSE}
system("aws s3 ls s3://ascstore/flatfield/ --recursive | awk '/IdentifySecondaryObjects.csv/' > ~/rapid/result_filelist.txt")
system("aws s3 ls s3://ascstore/metadata/ --recursive | awk '/.json/' > ~/rapid/metadata_filelist.txt")
system("aws s3 ls s3://ascstore/inbox/ --recursive > ~/rapid/inbox_filelist.txt")
```

I perform a double check and filter both results, metadata and raw data in the inbox.

```{r}
missing_result_files <- read_table2(here::here("result_filelist.txt"), 
    col_names = FALSE) %>% magrittr::set_colnames(c("date", "time", "size", "path")) %>% 
  mutate(id_observation = str_extract(path, pattern = "0000\\d+__\\d+-\\d\\d-\\d+T\\d+_\\d+_\\d+-Measurement_\\d-sk\\d-...-f..-ch\\d")) %>% 
  #mutate(size_filter = if_else(size < quantile(.99, size, na.rm = TRUE))) %>%
  anti_join(measurement %>% select(id_observation)) %>% 
  filter(size < 1000000) #>99% of all filesizes generally observed

nrow(missing_result_files)
```

No result files that have been generated are missing in our measurement overview.
Next, I check the available metadata for completeness.


```{r}
missing_metadata_files <- read_table2(here::here("metadata_filelist.txt"), 
    col_names = FALSE) %>% magrittr::set_colnames(c("date", "time", "size", "path")) %>% 
  mutate(id_measurement = str_extract(path, pattern = "0000\\d+__\\d+-\\d\\d-\\d+T\\d+_\\d+_\\d+-Measurement_\\d")) %>%
  mutate(id_barcode = str_extract(path, pattern = "0000\\d+"),
         channel = str_extract(path, pattern = "_[a-z][a-z]_") %>% substr(2,3),
         well = str_extract(path, pattern = "[A-Z]\\d+.json") %>% substr(1,3)) %>% 
  # only keeping barcodes I am interested in
  filter(id_barcode %in% barcodes) %>%
  #count(id_measurement, well) %>% 
  anti_join(measurement %>% select(id_measurement, well, id_barcode) %>% distinct()) %>% 
  left_join(measurement %>% select(id_measurement, contains("no")) %>% distinct())

missing_metadata_files %>% arrange(id_measurement, well, channel) %>% 
  select(id_measurement, well, channel) %>% 
  count(channel)
```

It turns out that missing measurements jobs only exist for 3 of the 4 job .json files available. The phase contrast jobs were mostly processed and generated a report! I believe this is due to the systems function, but will have to check in detail.

```{r}
inbox_files <- read_table2(here::here("inbox_filelist.txt"), 
    col_names = FALSE) %>% magrittr::set_colnames(c("date", "time", "size", "path")) %>% 
  mutate(id_measurement = str_extract(path, pattern = "0000\\d+__\\d+-\\d\\d-\\d+T\\d+_\\d+_\\d+-Measurement_\\d")) %>% 
  mutate(row = str_extract(path, pattern = "r\\d+") %>% substr(2,3) %>% as.numeric() %>% LETTERS[.],
         col = str_extract(path, pattern = "c\\d+") %>% substr(2,3),
         well = paste0(row, col),
         channel = str_extract(path, pattern = "ch\\d")) %>% 
  filter(well != "NANA")

missing_inbox_files <- inbox_files %>% 
  distinct(id_measurement, well, channel) %>% arrange(id_measurement, well) %>%
  # extracting the barcode
  mutate(id_barcode = str_extract(id_measurement, pattern = "0000\\d+")) %>% 
  # only keeping barcodes I am interested in
  filter(id_barcode %in% barcodes) %>%
  anti_join(measurement %>% select(id_measurement, well, id_barcode) %>% distinct()) %>%
  # adding some handy metadata for plotting
  left_join(measurement %>% select(id_measurement, id_barcode, contains("no")) %>% distinct()) 

missing_inbox_files %>% arrange(id_measurement, well, channel) %>% 
  select(id_measurement, well, channel) %>% 
  count(channel)
```

I plot the files on both the inbox and metadata level that have not been processed yet. Note, however, that a larger portion of expected files simply does not exist on AWS S3! 

```{r}
df <- missing_inbox_files %>% 
  mutate(source = "inbox") %>%
  rbind(missing_metadata_files %>% select(id_measurement:iteration_no) %>% mutate(source = "metadata")) %>%
  unite(run, measurement_no, iteration_no, remove = FALSE) %>%
  count(run, id_barcode, measurement_no,well, source) %>% 
  mutate(row = substr(well, 1, 1) %>% match(., LETTERS[26:1]),
         col = substr(well, 2, 3) %>% as.numeric()) %>% 
  filter(!(source == "inbox" & n != 4))

df %>%
  #filter(source == "inbox") %>%
  ggplot(aes(col, row, fill = source)) + 
  geom_tile(alpha = 0.7) + 
  scale_fill_brewer(type = "qual") +
  facet_grid(run~id_barcode) + 
  ggtitle("Missing wells that have been imaged during pilot phase") + 
  theme_classic() +
  theme(axis.text.x = element_blank()) + 
  theme(axis.text.y = element_blank()) + 
  theme(legend.position = "bottom") + 
  labs(fill = "Number of channels available in inbox")
```

Based on this analysis, I conclude: 
* Most missing images for 000012094903 were not completely uploaded, they are simply missing. 
* Some (visible in the plot above) missing images for 000012094903 were uploaded, but never registered during metadata generation. This might be an issue with s3fs. 
* Missing images for 000012048903 were uploaded, exist in the inbox, are complete and were registered but never processed. This might be an issue with the cluster at some point.

In order to collect data for these missing wells, I will need to re-generate metadata for selected plates and resubmit processing jobs. I focus on 000012048903 as this is the biggest possible gain vs. input relationship. 

# Data Filtering

I extract the starting time for each plate using the exact timestamp. 

```{r}
measurement <- measurement %>% 
  # I drop NA entries, they are corrupted
  drop_na() %>%
  mutate(measurement_no = if_else(id_barcode == "000012048703" & measurement_no == 4, 2, measurement_no)) %>% 
  mutate(measurement_no = if_else(id_barcode == "000012048703" & measurement_no == 6, 3, measurement_no)) %>% 
  # extracting date
  mutate(date = str_extract(string = id_measurement, pattern = "20\\d\\d-\\d\\d-\\d\\dT\\d\\d_\\d\\d_\\d\\d") %>% 
           str_replace(pattern = "T", replacement = "-") %>% lubridate::ymd_hms())
  
```

Not every sample has 3 measurments on subsequent days. It will be hard to compare them. For now I build the analysis in a way that is only focused on the wall-clock imaging time.

```{r}
measurement %>% 
  select(id_barcode, date, measurement_no) %>% distinct() %>% arrange(date)
```

I fix some measurment_no indicators.

```{r}
measurement = measurement %>% 
  mutate(measurement_no = case_when(id_barcode == "000012049003" & date == ymd_hms("2019-01-28 21:02:07") ~ 1,
                                    id_barcode == "000012049003" & date == ymd_hms("2019-01-29 19:11:28") ~ 2,
                                    id_barcode == "000012049003" & date == ymd_hms("2019-01-31 18:40:09") ~ 3,
                                    id_barcode == "000012049003" & date == ymd_hms("2019-02-01 12:09:07") ~ 4,
                                    TRUE ~ measurement_no))
```

I plot the plate layout again.

```{r}
df <- measurement %>% 
  unite(run, measurement_no, iteration_no, remove = FALSE) %>%
  select(run, id_barcode, measurement_no,well) %>% 
  mutate(row = substr(well, 1, 1) %>% match(., LETTERS[26:1]),
         col = substr(well, 2, 3) %>% as.numeric()) %>% 
  mutate(status = 1)

df %>%
  ggplot(aes(col, row)) + 
  geom_tile(color = "black") + 
  facet_grid(run~id_barcode) + 
  ggtitle("Imaged wells during pilot phase, corrected") + 
  theme_classic() +
  theme(axis.text.x = element_blank()) + 
  theme(axis.text.y = element_blank())
```

# Time imputations

I impute time based on the following assumptions: 
* the measurement timepoint in the barcode reflects the time when a measurement was started
* there is a constant time that is needed for well completion
* each measurement starts at well P01 and ends at well A24
* in some cases, measurement protocols have implemented a waiting time between each imaging round (multiple imaging rounds per measurement with a t>0s gap between the rounds. I don't think this is a good idea, but it has been done.) 

I manually annotate the imaging protocols for each plate in our dataset.

```{r}
anno <- tibble(id_barcode = barcodes[1:3],
       library = "XXXX_2018/11/06",
       protocol = "ascites_mush_40x_mito_fitc_singlecell_z-2_lowerexp_fullplate_6h") %>% 
rbind(tibble(id_barcode = barcodes[4:6],
       library = "9375_2019/04/03",
       protocol = "ascites_mush_40x_mito_fitc_singlecell_z-2_lowerexp_fullplate_6h_with_break")) %>% 
  cbind(line = c("CCLF_cRCRF1066T",
                 "CCLF_cRCRF1047T",
                 "CCLF_cRCRF1047T",
                 "CCLF_cRCRF1047T",
                 "CCLF_cRCRF1050T",
                 "CCLF_cRCRF1068T"))

```

I now create a manual well-wise annotation file for every barcode & measurement using the plate annotation as the basis.

```{r}
measurement %>% select(contains("id"), well, date, contains("_no")) %>% select(-contains("observation")) %>% 
  distinct() %>% left_join(plate_anno) %>% 
    mutate(row = substr(well, 1, 1) %>% match(., LETTERS),
         col = substr(well, 2, 3) %>% as.numeric()) %>%
  # joining anno file and formatting annotation according to the written/online documentation
  mutate(sample = case_when(id_barcode == "000012048703" & section == 1 ~ "CD45-",
                            id_barcode == "000012048703" & section == 2 ~ "raw",
                            id_barcode == "000012048703" & section == 2 ~ "empty",
                            # next plate
                            id_barcode == "000012048903" & section %in% c(1,2) ~ "CD45-",
                            id_barcode == "000012048903" & section == 3 ~ "CD45-_raw_1:1",
                            # next plate
                            id_barcode == "000012049003" & section %in% c(1,2) ~ "CD45-",
                            id_barcode == "000012049003" & section == 3 ~ "CD45-_raw_1:1",
                            # next plate, processing Mushriq's
                            id_barcode == "000012094903" & section %in% c(1,2) ~ "CD45-",
                            id_barcode == "000012094903" & section == 3 ~ "empty",
                            # next plate
                            id_barcode == "000012094903" & section %in% c(1,2) ~ "CD45-",
                            id_barcode == "000012094903" & section == 3 ~ "empty",
                            
                            
         flag = case_when(id_barcode == "000012048703" & well %in% c("K16", "M16", "N13", "N14", 
                                                                     "N15", "N16", "P13", "P14", "P15", "P16") ~ TRUE,
                          
                          TRUE ~ FALSE)
           
```


```{r}
library(lubridate)
s_per_plate <- hms("6:37:23") %>% lubridate::seconds() %>% as.numeric()
s_per_well <- s_per_plate/384 #time_per
s_measurment_interval <- hms("24:00:00") %>% lubridate::seconds() %>% as.numeric()

time_layout <- tibble(row = rep(c(LETTERS[c(16:1)], LETTERS[c(1:16)]), times = 12),
       col = rep(c(1:24), each = 16)) %>% 
  mutate(col = str_pad(col, 2, side = "left", pad = "0"),
         well = paste0(row, col),
         index = 1:384,
         time = index*s_per_well) %>% 
  dplyr::select(well, well_time = time)

raw_map(data = time_layout$well_time,
        well = time_layout$well,
        plate = 384) +
    ggtitle("Time layout for imaging protocol") +
    theme_dark() +
    scale_fill_viridis()
```

## Cell seeding

Now we want to apply this time scheme over the different timepoints. I also add the cell seeding pattern to the plate, which is unique for most plates. 

```{r}
index_1 <- index_1 %>%
  collect() %>%
  # cleaning timepoint names
  mutate(time_day = stringr::str_sub(plate, -1, -1) %>% as.numeric(),
         time_point = time_day*2,
         time_point = if_else(time == "sk1", time_point-1, time_point)) %>%
  # this module defines the additional time in seconds added per plate
  mutate(add_time = if_else(time_point %% 2 == 0, s_per_plate, 0),
         add_time = add_time + (time_day-1)*s_measurment_interval) %>%
  left_join(time_layout) %>% 
  mutate(real_time = well_time + add_time) %>%
  # seeding pattern 
  mutate(col = substr(well, 2, 3)) %>% 
  mutate(col = as.numeric(col),
         cell = case_when(col %in% c(1:8, 13:20) ~ "cd45_minus",
                          col %in% c(9:12, 21:24) ~ "cd45_minus_raw_mix")) %>% 
  # adding replicate anno 
  mutate(replicate = if_else(grepl(pattern = "12048903", plate), 2, 1))

index_2 <- index_2 %>%
  collect() %>%
  # cleaning timepoint names
  mutate(time_day = stringr::str_sub(plate, -1, -1) %>% as.numeric(),
         time_point = time_day*2,
         time_point = if_else(time == "sk1", time_point-1, time_point)) %>%
  # this module defines the additional time in seconds added per plate
  mutate(add_time = if_else(time_point %% 2 == 0, s_per_plate, 0),
         add_time = add_time + (time_day-1)*s_measurment_interval) %>%
  left_join(time_layout) %>% 
  mutate(real_time = well_time + add_time) %>%
  # seeding pattern 
  mutate(col = substr(well, 2, 3)) %>% 
  mutate(col = as.numeric(col),
         cell = case_when(col %in% c(1:8, 13:20) ~ "cd45_minus",
                          col %in% c(9:12, 21:24) ~ "cd45_minus_raw_mix")) %>% 
  # adding replicate anno 
  mutate(replicate = if_else(grepl(pattern = "12048903", plate), 2, 1))
```


After importing and annotating the data I create a shared object and start my exploration. 

```{r}
replicate_1_j <- replicate_1 %>%
  left_join(index_1, copy = TRUE)

replicate_2_j <- replicate_2 %>%
  left_join(index_2, copy = TRUE)
```

# EDA and QC

The object segmentation is currently not ideal. Therefore I remove objects that are clearly over or undersegmented. I evaluate the size distribution of segmented objects and exclude objects that have extreme values. 

I filter all extremely sized objects from the database.

```{r}
# show behaviour of viability over time for untreated and control samples
# show robustness of sample viability behaviour over time between replicates
# show similar patterns in compound response for both biological replciates 
get_area <- function(db){
  area <- list()
  
  area$distribution <- db %>% dplyr::select(area_shape_area, id, cell) %>%
  collect()
  
  area$area_percentile <- area$area_shape_area %>% quantile(probs = seq(0, 1, 0.01))
  
  area$min <- unname(area_percentile[2])
  area$max <- unname(area_percentile[100])
  
  return(area)
}

area_1 <- replicate_1_j %>% get_area()
#area_2 <- replicate_2_j %>% get_area()

# updating the database
replicate_1_j <- replicate_1_j %>%
  filter(area_shape_area < max & area_shape_area > min)

# # updating the database
replicate_2_j <- replicate_2_j %>%
  filter(area_shape_area < max & area_shape_area > min)

area_1$distribution %>% 
  filter(area_shape_area < area_1$max & area_shape_area > area_1$min) %>%
  ggplot(aes(area_shape_area, fill = cell)) + 
  geom_histogram(alpha = 0.3, position="identity") + 
  theme_classic() + 
  #facet_wrap(~ cell) + 
  scale_fill_brewer(type = "qual") +
  labs(title = "Size Distribution of segmented objects",
       subtitle = "after removing 1% and 99% percentile",
       caption = "1047, replicate 1") +
  theme(legend.position = "None") +
area_2$distribution %>%
  filter(area_shape_area < area_2$max & area_shape_area > area_2$min) %>%
  ggplot(aes(area_shape_area, fill = cell)) +
  geom_histogram(alpha = 0.3, position="identity") +
  theme_classic() +
  #facet_wrap(~ cell) +
  scale_fill_brewer(type = "qual") +
  labs(caption = "1047, replicate 2") +
  ggsave("size_histogram.png")


```

I am wondering if the number of objects stays the same over time and across wells. I calculate QC threshold values for each batch that remove wells, if the number of objects is unexpectedly large.

```{r}
get_cell_count <- function(db){
  cell_count <- list()
  
  cell_count$distribution <- db %>% 
    dplyr::select(well, cell, time_point, plate) %>% 
    count(well, cell, time_point, plate) %>% 
    collect()
  
  cell_count$threshold <- cell_count$distribution %>%
  ungroup() %>%
  group_by(time_point) %>% 
  summarise(sd = sd(n),
            mean = mean(n),
            min = mean-3*sd,
            max = mean+3*sd#,
            #min = quantile(n, probs = seq(0, 1, 0.01))[2],
            #max = quantile(n, probs = seq(0, 1, 0.01))[100]
            )
  
  return(cell_count)
}

cell_count_1 <- replicate_1_j %>% get_cell_count()
cell_count_2 <- replicate_2_j %>% get_cell_count()

# plot_cell_count <- function(cell_count){
#   cell_count
# }

cell_count_1$distribution %>%
  ungroup() %>%
  mutate(well = factor(well) %>% fct_inorder()) %>%
  ggplot(aes(well, n, color = cell)) + 
  geom_point(alpha = 0.8) + 
  geom_smooth(color = "red") +
  geom_hline(data = cell_count_1$threshold, aes(yintercept = min), linetype = "dashed") + 
  geom_hline(data = cell_count_1$threshold, aes(yintercept = max), linetype = "dashed") + 
  facet_wrap(~ time_point, ncol = 2) + 
  theme(axis.text.x = element_blank(),
        legend.position = "None") + 
  #scale_y_log10() + 
  scale_color_brewer() +
  labs(title = "Number of objects and their distribution across the plate",
       subtitle = "shown are well A01 to P24",
       caption = "1047, replicate 2") + 
cell_count_2$distribution %>%
  ungroup() %>%
  mutate(well = factor(well) %>% fct_inorder()) %>%
  ggplot(aes(well, n, color = cell)) + 
  geom_point(alpha = 0.8) + 
  geom_smooth(color = "red") +
  geom_hline(data = cell_count_2$threshold, aes(yintercept = min), linetype = "dashed") + 
  geom_hline(data = cell_count_2$threshold, aes(yintercept = max), linetype = "dashed") + 
  facet_wrap(~ time_point, ncol = 2) + 
  theme(axis.text.x = element_blank()) + 
  #scale_y_log10() + 
  scale_color_brewer() +
  labs(#title = "Number of objects and their distribution across the plate",
       #subtitle = "shown are well A01 to P24",
       caption = "1047, replicate 2") + 
  ggsave("count_threshold.png", width = 8, height = 4)
```

It seems as if not all measurment data was gathered: 

* Replicate 1 is missing timepoint 8. This might be because the microscope was running out of storage space and only one field of view was collected for timepoint 7. That also explains the small number of cells per well for timepoint 7. I have to be careful to sample cells in a balanced way from these datasets.
* Replicate 2 is missing measurment data at timepoints 3 and 4. It turns out that these images were processed but for some reason the measurment data was not ingested into the database. 

Based on these cutoff values I am removing wells that do not pass the QC 

```{r}
remove_wells_count <- function(index_df, cell_count){
flag_id <- cell_count$threshold %>% 
  dplyr::select(time_point, min, max) %>% 
  left_join(., cell_count$distribution) %>% 
  mutate(flag_count = if_else(n > max | n < min, TRUE, FALSE)) %>% 
  #count(flag_count)
  filter(flag_count == TRUE) %>% 
  dplyr::select(plate, well) %>% 
  left_join(index_df %>% dplyr::select(plate, well, id) %>% distinct()) %>% 
  .$id

flag_id_keep <- index_df %>% filter(!(id %in% flag_id)) %>% .$id %>% unique()

return(flag_id_keep)
}

replicate_1_j <- replicate_1_j %>% 
  filter((id %in% remove_wells_count(index_1, cell_count_1)))

replicate_2_j <- replicate_2_j %>% 
  filter((id %in% remove_wells_count(index_2, cell_count_2)))
```

I k

```{r}
dye_intensity <- replicate_2_j %>% 
  dplyr::select(intensity_integrated_intensity_ch4, intensity_integrated_intensity_ch3, cell, time_point) %>% 
  collect()

dye_cluster <- dye_intensity %>% 
  nest(intensity_integrated_intensity_ch3, 
       intensity_integrated_intensity_ch4) %>% 
  mutate(kmeans = purrr::map(data, ~ .x %>% as.matrix %>% kmeans(centers = 2)),
         cluster = purrr::map(kmeans, ~ .x %>% .$cluster)) %>% 
  unnest(cluster)

dye_intensity %>% 
  left_join(dye_cluster) 

get_dye_intensity <- function(db){
  dye_intensity <- db %>% 
  dplyr::select(intensity_integrated_intensity_ch4, intensity_integrated_intensity_ch3, cell, time_point, id) %>% 
  collect()

dye_cluster <- dye_intensity %>% 
  nest(intensity_integrated_intensity_ch3, 
       intensity_integrated_intensity_ch4) %>% 
  mutate(kmeans = purrr::map(data, ~ .x %>% as.matrix %>% kmeans(centers = 2)),
         cluster = purrr::map(kmeans, ~ .x %>% .$cluster)) %>% 
  unnest(cluster)

dye_intensity %>% 
  left_join(dye_cluster) %>% 
  return()
}

dye_intensity_2 <- get_dye_intensity(replicate_2_j)

%>%
  ggplot(aes(intensity_integrated_intensity_ch4, color = cluster)) + 
  geom_histogram() +
  theme_bw() + 
  facet_grid(plate ~ time) + 
  scale_x_log10() + 
  geom_vline(xintercept = 500)

# replicate_1_j %>% 
#   select(intensity_integrated_intensity_ch4, well, time, plate) %>%
#   filter(intensity_integrated_intensity_ch4 < 100 & intensity_integrated_intensity_ch4 > 1000) %>%
#   group_by()
```


```{r}
replicate_1_j %>% 
  #select(intensity_integrated_intensity_ch4, intensity_integrated_intensity_ch3, well, time, plate) %>% 
  ggplot(aes(intensity_integrated_intensity_ch4)) + 
  geom_histogram() +
  theme_bw() + 
  facet_grid(plate ~ time) + 
  scale_x_log10()
```


```{r}
replicate_1_j %>% 
  #select(intensity_integrated_intensity_ch4, intensity_integrated_intensity_ch3, well, time, plate) %>% 
  ggplot(aes(intensity_integrated_intensity_ch4, intensity_integrated_intensity_ch3)) + 
  geom_hex() +
  theme_bw() + 
  facet_grid(plate ~ time) + 
  scale_x_log10() + 
  scale_y_log10() + 
  scale_fill_viridis()
```


```{r}
max <- area_percentile[100] %>% unname()
min <- area_percentile[2] %>% unname()

replicate_1_jg <- replicate_1_j %>% 
  #removing outliers
  dplyr::filter(area_shape_area < max & area_shape_area > min) %>%
  group_by(well, time) %>% 
  summarise_at(vars(area_shape_area:texture_variance_ch4_3_03), funs(mean, var, n), na.rm = TRUE)  %>% collect()
```

# Quick and Dirty cutoff 

```{r}
replicate_1_jd <- replicate_1_j %>% 
  #removing outliers
  dplyr::filter(area_shape_area < max & area_shape_area > min) %>% 
  mutate(alive = if_else(intensity_integrated_intensity_ch4 > 500, 1, 0))

replicate_1_jdr <- replicate_1_jd %>% 
  group_by(well, time, plate) %>%
  summarise(n = n(),
            sum = sum(alive)) %>% 
  mutate(prop = sum/n)

viability <- replicate_1_jdr %>% 
  #head(100) %>% 
  collect()
```

```{r}
viability %>% 
  ggplot(aes(prop, n)) + 
  geom_smooth(method = "lm") + 
  geom_point(alpha = 0.3) +
  theme_bw() + 
  facet_grid(plate ~ time) + 
  scale_y_log10()
  
```

```{r}
viability_anno <- viability %>% 
  left_join(plate_anno) %>% 
  ungroup() #%>%
 # group_by(compound_name, concentration, time_point) %>% 
 # summarise(n = sum(n),
     #       sum = sum(sum),
    #        prop = sum/n)
```

```{r}
viability_anno %>%
  #filter(compound_name == "Trametinib") %>% 
  ggplot(aes(time_point, prop, color = concentration, group = concentration)) + 
  geom_point() + 
  geom_line(aes(group = concentration)) +
  theme_bw() + 
  facet_wrap(~compound_name)
```


```{r}
viability_anno %>%
  filter(compound_name %in% c("DMSO", "Boretzomib")) %>% 
  ggplot(aes(time_point, prop, color = compound_name, group = concentration)) + 
  geom_point() + 
  geom_smooth(aes(group = compound_name))+
 #geom_line(aes(group = compound_name)) +
  theme_bw() #+ 
  #facet_wrap(~compound_name)
```

```{r}
viability_anno %>%
  #filter(compound_name == "Afatinib") %>% 
  ggplot(aes(concentration, prop, color = time_point, group = time_point)) + 
  geom_point() + 
  geom_line(aes(group = time_point)) +
  theme_bw() + 
  facet_wrap(~compound_name) + 
  scale_x_reverse()
```


We can see a slight shift of estimated proportions dependent on the day of measurment and the number of observations. We should apply an emperical bayes estimator to account for both effects that impact proportions.

# UMAP

I define UMAP parameters for efficient clustering 

```{r}
umap_settings_cluster <- umap.defaults
# according to https://umap-learn.readthedocs.io/en/latest/clustering.html we change our parameters
umap_settings_cluster$n_neighbors <- 30
umap_settings_cluster$min_dist <- 0
```

I create a sample from the dataset

```{r}
set.seed(3423)

sample_index <- index_1 %>% collect() %>% 
  group_by(well, time, plate) %>% 
  sample_n(10) %>% 
  ungroup() %>%
  dplyr::select(id, object_number, image_number)


sample <- replicate_1_j %>% 
  #removing outliers
  dplyr::filter(area_shape_area < max & area_shape_area > min) %>% 
  left_join(sample_index,., copy = TRUE) %>% 
  drop_na()
```



I create a UMAP embedding

```{r}
no_umap_sample <- sample %>% 
  dplyr::select(everything(),
                -contains("location"),
                -contains("_x"),
                -contains("_y"),
                -id, -plate, -time, -well, -field, -channel,
                -number_object_number,
                -parent_identify_primary_objects,
                -image_number,
                -object_number,
                -area_shape_euler_number,
                -area_shape_center_z) %>% 
  as.data.frame() %>%
  #mutate_all(funs(as.numeric)) %>%
  as.matrix()

no_umap_sample_bf <- no_umap_sample %>% 
  as.data.frame() %>%
  dplyr::select(everything(), 
                -contains("ch3"),
                -contains("ch4")) %>% 
  mutate_all(funs(as.numeric)) %>%
  as.matrix()

umap_sample <- no_umap_sample %>%
  umap(config = umap_settings_cluster)

saveRDS(umap_sample, "umap_sample.Rdata")

umap_sample_bf <- no_umap_sample_bf %>%
  umap(config = umap_settings_cluster)

saveRDS(umap_sample_bf, "umap_sample_bf.Rdata")
```

Do I need a UMAP embedding? 

```{r, eval = FALSE}
cluster_no_umap <- no_umap_sample %>% 
  dbscan::hdbscan(minPts = 200)

saveRDS(cluster_no_umap, "cluster_no_umap.Rdata")
```

After running the embedding, we see how most cells can actually not be assigned to a given cluster. Let's try applying a UMAP nonlinearity.

```{r}
readRDS("cluster_no_umap.Rdata")
```

I run a quick QC of the embedding. 

```{r}
umap_sample$layout %>% as_tibble() %>% cbind(sample,.) %>%
  ggplot(aes(V1, V2, color = intensity_integrated_intensity_ch3)) + 
  geom_point(alpha = 0.3) + 
  facet_grid(plate ~ time) + 
  theme_bw() + 
  scale_color_viridis()
  
  
cluster <- umap_sample$layout %>% 
  dbscan::hdbscan(minPts = 200)

umap_sample$layout %>% as_tibble() %>% cbind(cluster$cluster,.) %>%
  magrittr::set_colnames(c("cluster", "V1", "V2")) %>%
  mutate(cluster = factor(cluster)) %>%
  cbind(sample,.) %>%
  ggplot(aes(V1, V2, color = cluster)) + 
  geom_point(alpha = 0.3) + 
  facet_grid(plate ~ time) + 
  theme_bw()
```


```{r}
umap.learn.predict
```


I also create an embedding only based on brightfield data 


```{r}
umap_sample_bf$layout %>% as_tibble() %>% cbind(sample,.) %>%
  ggplot(aes(V1, V2, color = intensity_integrated_intensity_ch4)) + 
  geom_point(alpha = 0.3) + 
  facet_grid(plate ~ time) + 
  theme_bw() + 
  scale_color_viridis()
  
  
cluster_bf <- umap_sample_bf$layout %>% 
  dbscan::hdbscan(minPts = 200)

umap_sample_bf$layout %>% as_tibble() %>% cbind(cluster_bf$cluster,.) %>%
  magrittr::set_colnames(c("cluster", "V1", "V2")) %>%
  mutate(cluster = factor(cluster)) %>%
  cbind(sample,.) %>%
  ggplot(aes(V1, V2, color = cluster)) + 
  geom_point(alpha = 0.3) + 
  facet_grid(plate ~ time) + 
  theme_bw()
```

I decide to continue my analysis with only brightfield images. It turns out that the features from brightfield images carry a lot of information and -in addition- are more stable over time. Cell Event and TMRM are not stable over time.

On the other hand, we don't know if the brightfield derived features are just not expressive enough and the reduction of Cell Event is actually driven by biology, rather than simple bleaching or degradation over time. 

As a consequence, it might be best to generate both projections for the dataset.

```{r}
id_of_interest = "000012048903__2019-02-06T20_33_15-Measurement_2-sk1-A01-f01-ch1"

transfer_umap <- function(id_of_interest, bf_umap_object, umap_object){
# collect data
umap_df <- replicate_1_j %>% 
  dplyr::filter(area_shape_area < max & area_shape_area > min) %>% 
  filter(id %in% id_of_interest) %>%
  dplyr::select(everything(),
                -contains("location"),
                -contains("_x"),
                -contains("_y"),
                -(image_number:object_number),
                -number_object_number,
                -parent_identify_primary_objects,
                -(id:channel),
                -area_shape_euler_number,
                -area_shape_center_z) %>% collect() %>%
  distinct()

# matrix
umap_matrix <- umap_df %>%
  as.matrix()

# matrix brightfield
umap_matrix_bf <- umap_df %>% 
  dplyr::select(everything(), 
                -contains("ch3"),
                -contains("ch4")) %>% 
  as.matrix()

# umap brightfield
output_bf <- predict(bf_umap_object, umap_matrix_bf) %>% 
  janitor::clean_names() %>% 
  magrittr::set_colnames(paste0(colnames(.), "_bf"))

# umap
output <- predict.umap(umap_object, umap_matrix) %>% 
  janitor::clean_names()
  
}
  
```


```{r}
tmp <- replicate_1_j %>% 
  dplyr::filter(area_shape_area < max & area_shape_area > min) %>%
  dplyr::filter(time == "sk1", plate == "000012048903__2019_02_05T20_27_41_Measurement_1") %>% 
  dplyr::select(area_shape_area:texture_variance_ch4_3_03) %>% 
  collect()
```


# Evaluate Afatinib + DMSO UMAP embedding 


For the sake of this demo, I will overwrite my sample and only focus on cells that were treated with Afatinib. 

```{r}
afatinib_index <- index_1 %>% collect() %>% 
  left_join(plate_anno) %>% 
  mutate(time_point = stringr::str_sub(plate, -1, -1) %>% as.numeric(),
         time_point = time_point*2,
         time_point = if_else(time == "sk1", time_point-1, time_point)) %>% 
  filter(compound_name %in% c("Afatinib", "DMSO")) %>% 
  .$id

sample <- replicate_1_j %>% 
  #removing outliers
  dplyr::filter(area_shape_area < max & area_shape_area > min) %>% 
  dplyr::filter(id %in% afatinib_index) %>%
  collect()
```



```{r}
no_umap_sample <- sample %>% 
  dplyr::select(everything(),
                -contains("location"),
                -contains("_x"),
                -contains("_y"),
                -id, -plate, -time, -well, -field, -channel,
                -number_object_number,
                -parent_identify_primary_objects,
                -image_number,
                -object_number,
                -area_shape_euler_number,
                -area_shape_center_z) %>% 
  as.data.frame() %>%
  #mutate_all(funs(as.numeric)) %>%
  as.matrix()

no_umap_sample_bf <- no_umap_sample %>% 
  as.data.frame() %>%
  dplyr::select(everything(), 
                -contains("ch3"),
                -contains("ch4")) %>% 
  mutate_all(funs(as.numeric)) %>%
  as.matrix()

umap_sample_afa_dmso <- no_umap_sample_bf %>%
  umap(config = umap_settings_cluster)

saveRDS(umap_sample_afa_dmso, "umap_sample_dmso_afatinib.Rdata")
```


```{r}
umap_sample_afa_dmso$layout %>% as_tibble() %>% #cbind(cluster_bf$cluster,.) %>%
  magrittr::set_colnames(c("V1", "V2")) %>%
  #mutate(cluster = factor(cluster)) %>%
  cbind(sample %>% dplyr::select(well, time, plate, intensity_integrated_intensity_ch4),.) %>%
  left_join(plate_anno) %>% 
  mutate(time_point = stringr::str_sub(plate, -1, -1) %>% as.numeric(),
         time_point = time_point*2,
         time_point = if_else(time == "sk1", time_point-1, time_point)) %>% 
  ungroup() %>%
  ggplot(aes(V1, V2, color = intensity_integrated_intensity_ch4)) + 
  geom_point(alpha = 0.1) + 
  #geom_density_2d() +
  facet_grid(time_point ~ concentration) + 
  theme_bw() + 
  scale_color_viridis()
```


